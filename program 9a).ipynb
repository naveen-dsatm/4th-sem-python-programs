{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set the URL of the first XKCD comic\n",
    "url = 'https://xkcd.com/1/'\n",
    "\n",
    "# Create a folder to store the comics\n",
    "if not os.path.exists('xkcd_comics'):\n",
    "    os.makedirs('xkcd_comics')\n",
    "\n",
    "# Loop through all the comics\n",
    "while True:\n",
    "    # Download the page content\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status()\n",
    "\n",
    "    # Parse the page content using BeautifulSoup\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "    # Find the URL of the comic image\n",
    "    comic_elem = soup.select('#comic img')\n",
    "    if comic_elem == []:\n",
    "        print('Could not find comic image.')\n",
    "    else:\n",
    "        comic_url = 'https:' + comic_elem[0].get('src')\n",
    "\n",
    "        # Download the comic image\n",
    "        print(f'Downloading {comic_url}...')\n",
    "        res = requests.get(comic_url)\n",
    "        res.raise_for_status()\n",
    "\n",
    "        # Save the comic image to the xkcd_comics folder\n",
    "        image_file = open(os.path.join('xkcd_comics', os.path.basename(comic_url)), 'wb')\n",
    "        for chunk in res.iter_content(100000):\n",
    "            image_file.write(chunk)\n",
    "        image_file.close()\n",
    "\n",
    "    # Get the URL of the previous comic\n",
    "    prev_link = soup.select('a[rel=\"prev\"]')[0]\n",
    "    if not prev_link:\n",
    "        break\n",
    "    url = 'https://xkcd.com' + prev_link.get('href')\n",
    "\n",
    "print('All comics downloaded.')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
